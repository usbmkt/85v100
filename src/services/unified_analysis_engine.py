#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ARQV30 Enhanced v2.0 - Unified Analysis Engine
Motor de an√°lise unificado que combina todas as capacidades
"""

import logging
import time
import json
import hashlib
from typing import Dict, List, Any, Optional
from datetime import datetime
from services.ai_manager import ai_manager
from services.unified_search_manager import unified_search_manager
from services.robust_content_extractor import robust_content_extractor
from services.pymupdf_client import pymupdf_client
from services.exa_client import exa_client
from services.mental_drivers_architect import mental_drivers_architect
from services.visual_proofs_generator import visual_proofs_generator
from services.anti_objection_system import anti_objection_system
from services.pre_pitch_architect import pre_pitch_architect
from services.archaeological_master import archaeological_master
from services.visceral_master_agent import visceral_master
from services.visual_proofs_director import visual_proofs_director
from services.forensic_cpl_analyzer import forensic_cpl_analyzer
from services.visceral_leads_engineer import visceral_leads_engineer
from services.pre_pitch_architect_advanced import pre_pitch_architect_advanced
from services.auto_save_manager import auto_save_manager, salvar_etapa, salvar_erro

logger = logging.getLogger(__name__))

class UnifiedAnalysisEngine:
    """Motor de an√°lise unificado com todas as capacidades"""

    def __init__(self):
        """Inicializa o motor unificado"""
        self.analysis_types = {
            'standard': 'An√°lise Padr√£o Ultra-Detalhada',
            'archaeological': 'An√°lise Arqueol√≥gica (12 Camadas)',
            'forensic_cpl': 'An√°lise Forense de CPL',
            'visceral_leads': 'Engenharia Reversa de Leads',
            'pre_pitch': 'Orquestra√ß√£o de Pr√©-Pitch',
            'complete': 'An√°lise Completa Unificada'
        }

        self.available_agents = {
            'arqueologist': archaeological_master,
            'visceral_master': visceral_master,
            'visual_director': visual_proofs_director,
            'drivers_architect': mental_drivers_architect,
            'anti_objection': anti_objection_system,
            'pre_pitch_architect': pre_pitch_architect,
            'forensic_cpl': forensic_cpl_analyzer,
            'visceral_leads': visceral_leads_engineer,
            'pre_pitch_advanced': pre_pitch_architect_advanced
        }

        self.required_categories = [
            "anti_objecao",
            "avatars",
            "completas",
            "concorrencia",
            "drivers_mentais",
            "funil_vendas",
            "insights",
            "metadata",
            "metricas",
            "palavras_chave",
            "pesquisa_web",
            "plano_acao",
            "posicionamento",
            "pre_pitch",
            "predicoes_futuro",
            "provas_visuais"
        ]

        logger.info("üöÄ Unified Analysis Engine inicializado")

    def execute_unified_analysis(
        self,
        data: Dict[str, Any],
        analysis_type: str = 'complete',
        session_id: str = None,
        progress_callback: Optional[callable] = None
    ) -> Dict[str, Any]:
        """Executa an√°lise unificada com tipo especificado"""

        logger.info(f"üöÄ Iniciando an√°lise unificada: {analysis_type}")
        start_time = time.time()

        # Inicia sess√£o se n√£o fornecida
        if not session_id:
            session_id = auto_save_manager.iniciar_sessao()

        # Salva in√≠cio da an√°lise
        salvar_etapa("analise_unificada_iniciada", {
            "data": data,
            "analysis_type": analysis_type,
            "session_id": session_id,
            "available_agents": list(self.available_agents.keys())
        }, categoria="analise_completa")

        try:
            if analysis_type == 'complete':
                # Executa an√°lise completa, garantindo todas as 16 categorias
                return self._execute_all_16_categories_and_validate(data, session_id, progress_callback)
            elif analysis_type == 'archaeological':
                return self._execute_archaeological_analysis(data, session_id, progress_callback)
            elif analysis_type == 'forensic_cpl':
                return self._execute_forensic_cpl_analysis(data, session_id, progress_callback)
            elif analysis_type == 'visceral_leads':
                return self._execute_visceral_leads_analysis(data, session_id, progress_callback)
            elif analysis_type == 'pre_pitch':
                return self._execute_pre_pitch_analysis(data, session_id, progress_callback)
            else:
                return self._execute_standard_analysis(data, session_id, progress_callback)

        except Exception as e:
            logger.error(f"‚ùå Erro na an√°lise unificada: {e}")
            salvar_erro("analise_unificada_erro", e, contexto=data)
            raise e

    def _execute_all_16_categories_and_validate(
        self,
        data: Dict[str, Any],
        session_id: str,
        progress_callback: Optional[callable] = None
    ) -> Dict[str, Any]:
        """Executa an√°lise completa com TODAS as 16 categorias obrigat√≥rias e valida√ß√£o rigorosa."""

        logger.info(f"üöÄ Iniciando an√°lise unificada COMPLETA (16 categorias) para sess√£o {session_id}")

        start_time = time.time()

        # Salva dados iniciais
        salvar_etapa("analise_unificada_inicio_16_categorias", {
            "session_id": session_id,
            "data_input": data,
            "timestamp": datetime.now().isoformat()
        }, categoria="analise_completa")

        # EXECU√á√ÉO OBRIGAT√ìRIA DAS 16 CATEGORIAS
        unified_analysis = self._execute_all_16_categories(data, session_id, progress_callback)

        # Valida√ß√£o RIGOROSA antes de finalizar
        validation_result = self._validate_completeness_16_categories(unified_analysis)

        if not validation_result['is_complete']:
            logger.error(f"‚ùå AN√ÅLISE INCOMPLETA: Categorias faltantes: {validation_result['missing_categories']}")
            # Re-executar categorias faltantes
            unified_analysis = self._complete_missing_categories(unified_analysis, validation_result['missing_categories'], data, session_id, progress_callback)

        processing_time = time.time() - start_time

        # Adiciona metadados finais
        unified_analysis['metadata'] = {
            'session_id': session_id,
            'generated_at': datetime.now().isoformat(),
            'processing_time_seconds': processing_time,
            'processing_time_formatted': f"{int(processing_time // 60)}m {int(processing_time % 60)}s",
            'version': '2.0.0',
            'categories_completed': 16,
            'analysis_id': session_id,
            'content_uniqueness_hash': self._generate_unique_hash(unified_analysis),
            'simulation_free': True,
            'analysis_completeness': 'TOTAL_16_CATEGORIES',
            'providers_used': len(unified_analysis.get('pesquisa_unificada', {}).get('provider_results', {})),
            'total_sources': unified_analysis.get('pesquisa_unificada', {}).get('statistics', {}).get('total_results', 0),
            'brazilian_sources': unified_analysis.get('pesquisa_unificada', {}).get('statistics', {}).get('brazilian_sources', 0),
            'exa_enhanced': exa_client.is_available(),
            'pymupdf_pro': pymupdf_client.is_available(),
        }

        # Salva an√°lise unificada final
        salvar_etapa("analise_unificada_final_16_categorias", unified_analysis, categoria="analise_completa")

        logger.info(f"‚úÖ An√°lise unificada COMPLETA (16 categorias) conclu√≠da em {processing_time:.2f}s")
        return unified_analysis

    def _execute_all_16_categories(
        self,
        data: Dict[str, Any],
        session_id: str,
        progress_callback: Optional[callable] = None
    ) -> Dict[str, Any]:
        """Executa TODAS as 16 categorias de an√°lise obrigat√≥rias de forma sequencial."""

        analysis_results = {}
        total_categories = len(self.required_categories)
        processed_count = 0

        # 1. Anti-Objecao
        if progress_callback: progress_callback(processed_count / total_categories, "1/16 - Construindo Sistema Anti-Obje√ß√£o...")
        try:
            objections_list = data.get('objections', [
                "N√£o tenho tempo para implementar isso agora",
                "Preciso pensar melhor sobre o investimento",
                "Meu caso √© muito espec√≠fico",
                "J√° tentei outras coisas e n√£o deram certo"
            ])
            avatar_data = data.get('avatar_visceral', {}) or self._get_fallback_avatar_data(data)
            analysis_results["anti_objecao"] = anti_objection_system.generate_complete_anti_objection_system(
                objections_list, avatar_data, data
            )
            logger.info("‚úÖ Categoria 'anti_objecao' conclu√≠da.")
        except Exception as e:
            logger.error(f"‚ùå Erro na categoria 'anti_objecao': {e}")
            analysis_results["anti_objecao"] = {"error": str(e)}
        processed_count += 1

        # 2. Avatars (usando dados do Visceral ou Arqueol√≥gico)
        if progress_callback: progress_callback(processed_count / total_categories, "2/16 - Criando Avatares Definitivos...")
        try:
            avatar_data_visceral = visceral_master.execute_visceral_analysis(data, session_id=session_id).get('avatar_visceral_ultra', {})
            avatar_data_arqueologico = archaeological_master.execute_archaeological_analysis(data, session_id=session_id).get('avatar_arqueologico_ultra', {})
            analysis_results["avatars"] = {
                'avatar_visceral': avatar_data_visceral,
                'avatar_arqueologico': avatar_data_arqueologico,
                'avatar_final_escolhido': avatar_data_visceral if avatar_data_visceral else avatar_data_arqueologico
            }
            logger.info("‚úÖ Categoria 'avatars' conclu√≠da.")
        except Exception as e:
            logger.error(f"‚ùå Erro na categoria 'avatars': {e}")
            analysis_results["avatars"] = {"error": str(e)}
        processed_count += 1

        # 3. Completas (Gerando a an√°lise principal que pode incluir outras)
        if progress_callback: progress_callback(processed_count / total_categories, "3/16 - Gerando An√°lise Completa Unificada...")
        try:
            # Reutiliza a l√≥gica existente para an√°lise completa, mas garante que ela seja executada
            analysis_results["completas"] = self._execute_complete_unified_analysis(data, session_id, progress_callback)
            logger.info("‚úÖ Categoria 'completas' conclu√≠da.")
        except Exception as e:
            logger.error(f"‚ùå Erro na categoria 'completas': {e}")
            analysis_results["completas"] = {"error": str(e)}
        processed_count += 1

        # 4. Concorr√™ncia
        if progress_callback: progress_callback(processed_count / total_categories, "4/16 - Analisando o Cen√°rio de Concorr√™ncia...")
        try:
            search_query = data.get('query') or f"concorr√™ncia {data.get('segmento', 'neg√≥cios')} Brasil"
            search_results = unified_search_manager.unified_search(search_query, max_results=15, context=data, session_id=session_id)
            analysis_results["concorrencia"] = {
                "pesquisa_concorrencia": search_results,
                "analise_concorrencial": ai_manager.generate_analysis(
                    f"Analise os seguintes resultados de busca sobre a concorr√™ncia no mercado de {data.get('segmento', 'neg√≥cios')}. Identifique os principais players, suas estrat√©gias de marketing, diferenciais e pontos fracos. Destaque oportunidades e amea√ßas. Use os dados: {json.dumps(search_results, ensure_ascii=False)[:10000]}",
                    max_tokens=2048
                )
            }
            logger.info("‚úÖ Categoria 'concorrencia' conclu√≠da.")
        except Exception as e:
            logger.error(f"‚ùå Erro na categoria 'concorrencia': {e}")
            analysis_results["concorrencia"] = {"error": str(e)}
        processed_count += 1

        # 5. Drivers Mentais
        if progress_callback: progress_callback(processed_count / total_categories, "5/16 - Criando Arsenal de Drivers Mentais...")
        try:
            avatar_data = analysis_results.get("avatars", {}).get("avatar_final_escolhido", {})
            analysis_results["drivers_mentais"] = mental_drivers_architect.generate_complete_drivers_system(avatar_data, data)
            logger.info("‚úÖ Categoria 'drivers_mentais' conclu√≠da.")
        except Exception as e:
            logger.error(f"‚ùå Erro na categoria 'drivers_mentais': {e}")
            analysis_results["drivers_mentais"] = {"error": str(e)}
        processed_count += 1

        # 6. Funil de Vendas
        if progress_callback: progress_callback(processed_count / total_categories, "6/16 - Mapeando o Funil de Vendas...")
        try:
            # Assume que 'completas' j√° executou a an√°lise do funil ou que podemos reexecutar
            funil_data = analysis_results.get("completas", {}).get("pre_pitch_invisivel", {}) or data.get('funil_vendas_data', {})
            analysis_results["funil_vendas"] = ai_manager.generate_analysis(
                f"Com base no contexto do projeto: {data.get('segmento', 'neg√≥cios')}, produto: {data.get('produto', 'N/A')}, e os dados do funil: {json.dumps(funil_data, ensure_ascii=False)[:10000]}, detalhe as etapas do funil de vendas, gargalos e otimiza√ß√µes necess√°rias.",
                max_tokens=2048
            )
            logger.info("‚úÖ Categoria 'funil_vendas' conclu√≠da.")
        except Exception as e:
            logger.error(f"‚ùå Erro na categoria 'funil_vendas': {e}")
            analysis_results["funil_vendas"] = {"error": str(e)}
        processed_count += 1

        # 7. Insights
        if progress_callback: progress_callback(processed_count / total_categories, "7/16 - Gerando Insights Estrat√©gicos...")
        try:
            # Reutiliza insights da an√°lise completa se dispon√≠vel, sen√£o gera novamente
            insights_data = analysis_results.get("completas", {}).get("insights_unificados", [])
            if not insights_data:
                search_query = data.get('query') or f"insights estrat√©gicos {data.get('segmento', 'neg√≥cios')} Brasil"
                search_results = unified_search_manager.unified_search(search_query, max_results=10, context=data, session_id=session_id)
                insights_data = ai_manager.generate_analysis(
                    f"Extraia os 20 insights mais valiosos e acion√°veis dos seguintes resultados de pesquisa para o mercado de {data.get('segmento', 'neg√≥cios')}. Use os dados: {json.dumps(search_results, ensure_ascii=False)[:10000]}",
                    max_tokens=2048
                )
            analysis_results["insights"] = {"insights_gerados": insights_data}
            logger.info("‚úÖ Categoria 'insights' conclu√≠da.")
        except Exception as e:
            logger.error(f"‚ùå Erro na categoria 'insights': {e}")
            analysis_results["insights"] = {"error": str(e)}
        processed_count += 1

        # 8. Metadata (J√° ser√° adicionado no final, mas podemos gerar um rascunho aqui)
        if progress_callback: progress_callback(processed_count / total_categories, "8/16 - Preparando Metadados...")
        try:
            # Metadados ser√£o consolidados no final, mas podemos adicionar informa√ß√µes preliminares se necess√°rio
            analysis_results["metadata"] = {
                'preliminary_generation_time': time.time() - start_time,
                'status': 'generating'
            }
            logger.info("‚úÖ Categoria 'metadata' (preliminar) conclu√≠da.")
        except Exception as e:
            logger.error(f"‚ùå Erro na categoria 'metadata': {e}")
            analysis_results["metadata"] = {"error": str(e)}
        processed_count += 1

        # 9. M√©tricas
        if progress_callback: progress_callback(processed_count / total_categories, "9/16 - Analisando M√©tricas Chave...")
        try:
            search_query = data.get('query') or f"m√©tricas de mercado {data.get('segmento', 'neg√≥cios')} Brasil"
            search_results = unified_search_manager.unified_search(search_query, max_results=10, context=data, session_id=session_id)
            analysis_results["metricas"] = {
                "pesquisa_metricas": search_results,
                "analise_metricas": ai_manager.generate_analysis(
                    f"Analise os dados de m√©tricas de mercado para o segmento de {data.get('segmento', 'neg√≥cios')}. Identifique KPIs importantes, benchmarks e tend√™ncias. Use os dados: {json.dumps(search_results, ensure_ascii=False)[:10000]}",
                    max_tokens=2048
                )
            }
            logger.info("‚úÖ Categoria 'metricas' conclu√≠da.")
        except Exception as e:
            logger.error(f"‚ùå Erro na categoria 'metricas': {e}")
            analysis_results["metricas"] = {"error": str(e)}
        processed_count += 1

        # 10. Palavras-Chave
        if progress_callback: progress_callback(processed_count / total_categories, "10/16 - Identificando Palavras-Chave Estrat√©gicas...")
        try:
            search_query = data.get('query') or f"palavras-chave {data.get('segmento', 'neg√≥cios')} Brasil"
            search_results = unified_search_manager.unified_search(search_query, max_results=10, context=data, session_id=session_id)
            analysis_results["palavras_chave"] = {
                "pesquisa_palavras_chave": search_results,
                "analise_palavras_chave": ai_manager.generate_analysis(
                    f"Com base na pesquisa de palavras-chave para {data.get('segmento', 'neg√≥cios')}, liste as 10 palavras-chave mais relevantes, com inten√ß√£o de compra clara e bom volume de busca. Detalhe o volume estimado e a concorr√™ncia. Use os dados: {json.dumps(search_results, ensure_ascii=False)[:10000]}",
                    max_tokens=2048
                )
            }
            logger.info("‚úÖ Categoria 'palavras_chave' conclu√≠da.")
        except Exception as e:
            logger.error(f"‚ùå Erro na categoria 'palavras_chave': {e}")
            analysis_results["palavras_chave"] = {"error": str(e)}
        processed_count += 1

        # 11. Pesquisa Web (J√° foi feita em outras categorias, mas pode ser consolidada aqui)
        if progress_callback: progress_callback(processed_count / total_categories, "11/16 - Consolidando Pesquisa Web...")
        try:
            # Reutiliza a pesquisa da categoria 'completas' se dispon√≠vel
            pesquisa_web_data = analysis_results.get("completas", {}).get("pesquisa_unificada", {})
            if not pesquisa_web_data:
                search_query = data.get('query') or f"pesquisa web {data.get('segmento', 'neg√≥cios')} Brasil"
                pesquisa_web_data = unified_search_manager.unified_search(search_query, max_results=20, context=data, session_id=session_id)
            analysis_results["pesquisa_web"] = pesquisa_web_data
            logger.info("‚úÖ Categoria 'pesquisa_web' conclu√≠da.")
        except Exception as e:
            logger.error(f"‚ùå Erro na categoria 'pesquisa_web': {e}")
            analysis_results["pesquisa_web"] = {"error": str(e)}
        processed_count += 1

        # 12. Plano de A√ß√£o
        if progress_callback: progress_callback(processed_count / total_categories, "12/16 - Elaborando Plano de A√ß√£o Detalhado...")
        try:
            # Reutiliza insights e dados de outras categorias para criar o plano
            insights = analysis_results.get("insights", {}).get("insights_gerados", "")
            drivers = analysis_results.get("drivers_mentais", {}).get("drivers_customizados", [])
            provas_visuais = analysis_results.get("provas_visuais", {}).get("provis_system", [])
            
            analysis_results["plano_acao"] = ai_manager.generate_analysis(
                f"Com base nos seguintes insights: {insights}, drivers mentais: {drivers}, e provas visuais: {provas_visuais}, crie um plano de a√ß√£o detalhado e sequencial para o projeto de {data.get('segmento', 'neg√≥cios')}. Inclua objetivos claros, atividades espec√≠ficas, prazos estimados e m√©tricas de sucesso. O objetivo principal √© {data.get('objetivo_geral', 'o crescimento do neg√≥cio')}.",
                max_tokens=3000
            )
            logger.info("‚úÖ Categoria 'plano_acao' conclu√≠da.")
        except Exception as e:
            logger.error(f"‚ùå Erro na categoria 'plano_acao': {e}")
            analysis_results["plano_acao"] = {"error": str(e)}
        processed_count += 1

        # 13. Posicionamento
        if progress_callback: progress_callback(processed_count / total_categories, "13/16 - Definindo Posicionamento Estrat√©gico...")
        try:
            # Reutiliza dados do avatar, drivers e an√°lise de concorr√™ncia
            avatar_data = analysis_results.get("avatars", {}).get("avatar_final_escolhido", {})
            drivers_data = analysis_results.get("drivers_mentais", {})
            concorrencia_data = analysis_results.get("concorrencia", {}).get("analise_concorrencial", "")

            analysis_results["posicionamento"] = ai_manager.generate_analysis(
                f"Com base no avatar: {avatar_data}, drivers mentais: {drivers_data}, e an√°lise de concorr√™ncia: {concorrencia_data}, defina um posicionamento de mercado √∫nico e irresist√≠vel para o produto/servi√ßo de {data.get('segmento', 'neg√≥cios')}. Crie uma proposta de valor clara e slogans impactantes. O objetivo √© {data.get('objetivo_geral', 'dominar o mercado')}.",
                max_tokens=2048
            )
            logger.info("‚úÖ Categoria 'posicionamento' conclu√≠da.")
        except Exception as e:
            logger.error(f"‚ùå Erro na categoria 'posicionamento': {e}")
            analysis_results["posicionamento"] = {"error": str(e)}
        processed_count += 1

        # 14. Pre-Pitch
        if progress_callback: progress_callback(processed_count / total_categories, "14/16 - Orquestrando o Pr√©-Pitch Invis√≠vel...")
        try:
            # Utiliza o agente espec√≠fico para pr√©-pitch avan√ßado
            selected_drivers = drivers_data.get('drivers_customizados', [])
            event_structure = data.get('event_structure', 'Webinar/Live/Evento')
            product_offer = data.get('product_offer', f"Produto: {data.get('produto', 'N/A')} - Pre√ßo: R$ {data.get('preco', 'N/A')}")

            analysis_results["pre_pitch"] = pre_pitch_architect_advanced.orchestrate_psychological_symphony(
                selected_drivers, avatar_data, event_structure, product_offer, session_id
            )
            logger.info("‚úÖ Categoria 'pre_pitch' conclu√≠da.")
        except Exception as e:
            logger.error(f"‚ùå Erro na categoria 'pre_pitch': {e}")
            analysis_results["pre_pitch"] = {"error": str(e)}
        processed_count += 1

        # 15. Predi√ß√µes Futuro
        if progress_callback: progress_callback(processed_count / total_categories, "15/16 - Prevendo Tend√™ncias Futuras...")
        try:
            search_query = data.get('query') or f"tend√™ncias futuro {data.get('segmento', 'neg√≥cios')} Brasil"
            search_results = unified_search_manager.unified_search(search_query, max_results=10, context=data, session_id=session_id)
            analysis_results["predicoes_futuro"] = ai_manager.generate_analysis(
                f"Com base nas tend√™ncias futuras para o mercado de {data.get('segmento', 'neg√≥cios')}, preveja os pr√≥ximos 3-5 anos. Identifique tecnologias emergentes, mudan√ßas de comportamento do consumidor e oportunidades disruptivas. Use os dados: {json.dumps(search_results, ensure_ascii=False)[:10000]}",
                max_tokens=2048
            )
            logger.info("‚úÖ Categoria 'predicoes_futuro' conclu√≠da.")
        except Exception as e:
            logger.error(f"‚ùå Erro na categoria 'predicoes_futuro': {e}")
            analysis_results["predicoes_futuro"] = {"error": str(e)}
        processed_count += 1

        # 16. Provas Visuais
        if progress_callback: progress_callback(processed_count / total_categories, "16/16 - Criando Provas Visuais Irrefut√°veis...")
        try:
            concepts_to_prove = self._extract_concepts_for_proofs(
                analysis_results.get("avatars", {}).get("avatar_final_escolhido", {}),
                analysis_results.get("drivers_mentais", {}),
                data
            )
            analysis_results["provas_visuais"] = visual_proofs_director.execute_provis_creation(
                concepts_to_prove,
                analysis_results.get("avatars", {}).get("avatar_final_escolhido", {}),
                analysis_results.get("drivers_mentais", {}),
                data,
                session_id
            )
            logger.info("‚úÖ Categoria 'provas_visuais' conclu√≠da.")
        except Exception as e:
            logger.error(f"‚ùå Erro na categoria 'provas_visuais': {e}")
            analysis_results["provas_visuais"] = {"error": str(e)}
        processed_count += 1

        return analysis_results

    def _validate_completeness_16_categories(self, analysis_results: Dict[str, Any]) -> Dict[str, Any]:
        """Valida se todas as 16 categorias obrigat√≥rias foram geradas corretamente."""
        missing_categories = []
        for category in self.required_categories:
            if category not in analysis_results or not analysis_results[category] or isinstance(analysis_results[category], dict) and "error" in analysis_results[category]:
                missing_categories.append(category)

        return {
            'is_complete': not missing_categories,
            'missing_categories': missing_categories
        }

    def _complete_missing_categories(
        self,
        current_analysis: Dict[str, Any],
        missing_categories: List[str],
        data: Dict[str, Any],
        session_id: str,
        progress_callback: Optional[callable] = None
    ) -> Dict[str, Any]:
        """Tenta reexecutar as categorias que falharam."""
        logger.warning(f"Tentando completar categorias faltantes: {missing_categories}")
        
        # Mapeia as categorias para suas respectivas fun√ß√µes de execu√ß√£o (simplificado aqui)
        category_execution_map = {
            "anti_objecao": lambda d, s, cb: anti_objection_system.generate_complete_anti_objection_system(
                d.get('objections', ["Fallback objection"]),
                d.get('avatar_visceral', {}),
                d
            ),
            "avatars": lambda d, s, cb: {
                'avatar_visceral': visceral_master.execute_visceral_analysis(d, session_id=s).get('avatar_visceral_ultra', {}),
                'avatar_arqueologico': archaeological_master.execute_archaeological_analysis(d, session_id=s).get('avatar_arqueologico_ultra', {}),
            },
            "concorrencia": lambda d, s, cb: {
                "analise_concorrencial": ai_manager.generate_analysis(
                    f"Reanalise a concorr√™ncia para {d.get('segmento', 'neg√≥cios')}. Use os dados: {json.dumps(unified_search_manager.unified_search(f'concorr√™ncia {d.get('segmento', 'neg√≥cios')} Brasil', max_results=15, context=d, session_id=s), ensure_ascii=False)[:10000]}",
                    max_tokens=2048
                )
            },
            "drivers_mentais": lambda d, s, cb: mental_drivers_architect.generate_complete_drivers_system(
                current_analysis.get("avatars", {}).get("avatar_final_escolhido", {}), d
            ),
            "funil_vendas": lambda d, s, cb: ai_manager.generate_analysis(
                f"Reanalise o funil de vendas para {d.get('segmento', 'neg√≥cios')}. Contexto: {json.dumps(current_analysis.get('pre_pitch', {}), ensure_ascii=False)[:10000]}",
                max_tokens=2048
            ),
            "insights": lambda d, s, cb: ai_manager.generate_analysis(
                 f"Reextracao de insights para {d.get('segmento', 'neg√≥cios')}. Use os dados: {json.dumps(unified_search_manager.unified_search(f'insights estrat√©gicos {d.get('segmento', 'neg√≥cios')} Brasil', max_results=10, context=d, session_id=s), ensure_ascii=False)[:10000]}",
                max_tokens=2048
            ),
            "metricas": lambda d, s, cb: {
                "analise_metricas": ai_manager.generate_analysis(
                    f"Reanalise as m√©tricas para {d.get('segmento', 'neg√≥cios')}. Use os dados: {json.dumps(unified_search_manager.unified_search(f'm√©tricas de mercado {d.get('segmento', 'neg√≥cios')} Brasil', max_results=10, context=d, session_id=s), ensure_ascii=False)[:10000]}",
                    max_tokens=2048
                )
            },
            "palavras_chave": lambda d, s, cb: {
                "analise_palavras_chave": ai_manager.generate_analysis(
                    f"Reanalise as palavras-chave para {d.get('segmento', 'neg√≥cios')}. Use os dados: {json.dumps(unified_search_manager.unified_search(f'palavras-chave {d.get('segmento', 'neg√≥cios')} Brasil', max_results=10, context=d, session_id=s), ensure_ascii=False)[:10000]}",
                    max_tokens=2048
                )
            },
            "pesquisa_web": lambda d, s, cb: unified_search_manager.unified_search(f'pesquisa web {d.get('segmento', 'neg√≥cios')} Brasil', max_results=20, context=d, session_id=s),
            "plano_acao": lambda d, s, cb: ai_manager.generate_analysis(
                f"Reelabore o plano de a√ß√£o para {d.get('segmento', 'neg√≥cios')} com base em insights atualizados. Insights: {current_analysis.get('insights', {}).get('insights_gerados', '')}, Drivers: {current_analysis.get('drivers_mentais', {}).get('drivers_customizados', [])}",
                max_tokens=3000
            ),
            "posicionamento": lambda d, s, cb: ai_manager.generate_analysis(
                f"Refine o posicionamento para {d.get('segmento', 'neg√≥cios')}. Avatar: {current_analysis.get('avatars', {}).get('avatar_final_escolhido', {})}, Concorr√™ncia: {current_analysis.get('concorrencia', {}).get('analise_concorrencial', '')}",
                max_tokens=2048
            ),
            "pre_pitch": lambda d, s, cb: pre_pitch_architect_advanced.orchestrate_psychological_symphony(
                current_analysis.get("drivers_mentais", {}).get('drivers_customizados', []),
                current_analysis.get("avatars", {}).get("avatar_final_escolhido", {}),
                d.get('event_structure', 'Webinar/Live/Evento'),
                d.get('product_offer', f"Produto: {d.get('produto', 'N/A')} - Pre√ßo: R$ {d.get('preco', 'N/A')}"),
                s
            ),
            "predicoes_futuro": lambda d, s, cb: ai_manager.generate_analysis(
                f"Reavalie as predi√ß√µes futuras para {d.get('segmento', 'neg√≥cios')}. Use os dados: {json.dumps(unified_search_manager.unified_search(f'tend√™ncias futuro {d.get('segmento', 'neg√≥cios')} Brasil', max_results=10, context=d, session_id=s), ensure_ascii=False)[:10000]}",
                max_tokens=2048
            ),
            "provas_visuais": lambda d, s, cb: visual_proofs_director.execute_provis_creation(
                self._extract_concepts_for_proofs(
                    current_analysis.get("avatars", {}).get("avatar_final_escolhido", {}),
                    current_analysis.get("drivers_mentais", {}),
                    d
                ),
                current_analysis.get("avatars", {}).get("avatar_final_escolhido", {}),
                current_analysis.get("drivers_mentais", {}),
                d,
                s
            )
            # 'completas' and 'metadata' are handled differently or will be recalculated
        }

        total_categories_to_retry = len(missing_categories)
        reprocessed_count = 0
        for category in missing_categories:
            if progress_callback: progress_callback((1.0/16.0) * (15.0 + reprocessed_count / total_categories_to_retry), f"Reexecutando {category}...")
            if category in category_execution_map:
                try:
                    current_analysis[category] = category_execution_map[category](data, session_id, progress_callback)
                    logger.info(f"‚úÖ Categoria '{category}' (reexecutada) conclu√≠da.")
                except Exception as e:
                    logger.error(f"‚ùå Erro ao reexecutar categoria '{category}': {e}")
                    current_analysis[category] = {"error": str(e)}
            else:
                logger.warning(f"Nenhuma fun√ß√£o de reexecu√ß√£o definida para a categoria: {category}")
            reprocessed_count += 1
        
        return current_analysis


    def _extract_unified_content(self, search_results: Dict[str, Any], session_id: str) -> Dict[str, Any]:
        """Extrai conte√∫do usando todos os extratores dispon√≠veis"""

        results = search_results.get('results', [])
        extracted_content = []
        pdf_content = []

        for i, result in enumerate(results[:15]):  # Top 15 resultados
            url = result.get('url', '')

            try:
                # Verifica se √© PDF
                if url.lower().endswith('.pdf') or 'pdf' in url.lower():
                    # Usa PyMuPDF Pro para PDFs
                    if pymupdf_client.is_available():
                        pdf_result = pymupdf_client.extract_from_url(url)
                        if pdf_result['success']:
                            pdf_content.append({
                                'url': url,
                                'title': result.get('title', ''),
                                'content': pdf_result['text'],
                                'metadata': pdf_result['metadata'],
                                'statistics': pdf_result['statistics'],
                                'extraction_method': 'PyMuPDF_Pro'
                            })
                            continue

                # Usa extrator robusto para p√°ginas web
                content, metadata = robust_content_extractor.extract_content(url)
                if content and len(content) > 200:
                    extracted_content.append({
                        'url': url,
                        'title': result.get('title', ''),
                        'content': content,
                        'source': result.get('source', 'unknown'),
                        'is_brazilian': result.get('is_brazilian', False),
                        'is_preferred': result.get('is_preferred', False),
                        'extraction_method': 'robust_extractor'
                    })

                # Delay para rate limiting
                time.sleep(0.3)

            except Exception as e:
                logger.error(f"‚ùå Erro ao extrair {url}: {e}")
                continue

        # Combina conte√∫do extra√≠do
        combined_content = {
            'web_content': extracted_content,
            'pdf_content': pdf_content,
            'statistics': {
                'total_web_pages': len(extracted_content),
                'total_pdf_pages': len(pdf_content),
                'total_content_length': sum(len(item['content']) for item in extracted_content + pdf_content),
                'extraction_success_rate': (len(extracted_content) + len(pdf_content)) / len(results) * 100 if results else 0
            }
        }

        # Salva conte√∫do extra√≠do
        salvar_etapa("conteudo_unificado_extraido", combined_content, categoria="pesquisa_web")

        return combined_content

    def _extract_concepts_for_proofs(
        self, 
        avatar_data: Dict[str, Any], 
        drivers_data: Dict[str, Any], 
        context_data: Dict[str, Any]
    ) -> List[str]:
        """Extrai conceitos que precisam de prova visual"""

        concepts = []

        # Conceitos do avatar
        if avatar_data.get('feridas_abertas_inconfessaveis'):
            concepts.extend(avatar_data['feridas_abertas_inconfessaveis'][:5])

        if avatar_data.get('sonhos_proibidos_ardentes'):
            concepts.extend(avatar_data['sonhos_proibidos_ardentes'][:5])

        # Conceitos dos drivers
        if drivers_data.get('drivers_customizados'):
            for driver in drivers_data['drivers_customizados'][:3]:
                concepts.append(driver.get('nome', 'Driver Mental'))

        # Conceitos gerais cr√≠ticos
        concepts.extend([
            "Efic√°cia do m√©todo",
            "Transforma√ß√£o real poss√≠vel",
            "ROI do investimento",
            "Diferencial da concorr√™ncia",
            "Tempo para resultados"
        ])

        return concepts[:12]  # M√°ximo 12 conceitos

    def _execute_archaeological_analysis(
        self,
        data: Dict[str, Any],
        session_id: str,
        progress_callback: Optional[callable] = None
    ) -> Dict[str, Any]:
        """Executa apenas an√°lise arqueol√≥gica"""

        if progress_callback:
            progress_callback(1, "üî¨ Iniciando an√°lise arqueol√≥gica...")

        # Pesquisa unificada
        search_query = data.get('query') or f"mercado {data.get('segmento', 'neg√≥cios')} Brasil 2024"
        search_results = unified_search_manager.unified_search(search_query, context=data, session_id=session_id)

        # An√°lise arqueol√≥gica
        archaeological_result = archaeological_master.execute_archaeological_analysis(
            data,
            research_context=json.dumps(search_results, ensure_ascii=False)[:15000],
            session_id=session_id
        )

        return {
            'tipo_analise': 'arqueologica',
            'projeto_dados': data,
            'pesquisa_unificada': search_results,
            'analise_arqueologica': archaeological_result,
            'metadata': {
                'analysis_type': 'archaeological',
                'session_id': session_id,
                'generated_at': datetime.now().isoformat()
            }
        }

    def _execute_forensic_cpl_analysis(
        self,
        data: Dict[str, Any],
        session_id: str,
        progress_callback: Optional[callable] = None
    ) -> Dict[str, Any]:
        """Executa an√°lise forense de CPL"""

        if progress_callback:
            progress_callback(1, "üî¨ Iniciando an√°lise forense de CPL...")

        transcription = data.get('transcription', '')
        context_data = {
            'contexto_estrategico': data.get('contexto_estrategico', ''),
            'objetivo_cpl': data.get('objetivo_cpl', ''),
            'formato': data.get('formato', ''),
            'temperatura_audiencia': data.get('temperatura_audiencia', '')
        }

        forensic_result = forensic_cpl_analyzer.analyze_cpl_forensically(
            transcription, context_data, session_id
        )

        return {
            'tipo_analise': 'forense_cpl',
            'projeto_dados': data,
            'analise_forense_cpl': forensic_result,
            'metadata': {
                'analysis_type': 'forensic_cpl',
                'session_id': session_id,
                'generated_at': datetime.now().isoformat()
            }
        }

    def _execute_visceral_leads_analysis(
        self,
        data: Dict[str, Any],
        session_id: str,
        progress_callback: Optional[callable] = None
    ) -> Dict[str, Any]:
        """Executa engenharia reversa de leads"""

        if progress_callback:
            progress_callback(1, "üß† Iniciando engenharia reversa visceral...")

        leads_data = data.get('leads_data', '')
        context_data = {
            'produto_servico': data.get('produto_servico', ''),
            'principais_perguntas': data.get('principais_perguntas', ''),
            'numero_respostas': data.get('numero_respostas', 0)
        }

        visceral_result = visceral_leads_engineer.reverse_engineer_leads(
            leads_data, context_data, session_id
        )

        return {
            'tipo_analise': 'visceral_leads',
            'projeto_dados': data,
            'engenharia_reversa_leads': visceral_result,
            'metadata': {
                'analysis_type': 'visceral_leads',
                'session_id': session_id,
                'generated_at': datetime.now().isoformat()
            }
        }

    def _execute_pre_pitch_analysis(
        self,
        data: Dict[str, Any],
        session_id: str,
        progress_callback: Optional[callable] = None
    ) -> Dict[str, Any]:
        """Executa orquestra√ß√£o de pr√©-pitch"""

        if progress_callback:
            progress_callback(1, "üéØ Iniciando orquestra√ß√£o de pr√©-pitch...")

        selected_drivers = data.get('selected_drivers', [])
        avatar_data = data.get('avatar_data', {})
        event_structure = data.get('event_structure', '')
        product_offer = data.get('product_offer', '')

        pre_pitch_result = pre_pitch_architect_advanced.orchestrate_psychological_symphony(
            selected_drivers, avatar_data, event_structure, product_offer, session_id
        )

        return {
            'tipo_analise': 'pre_pitch',
            'projeto_dados': data,
            'orquestracao_pre_pitch': pre_pitch_result,
            'metadata': {
                'analysis_type': 'pre_pitch',
                'session_id': session_id,
                'generated_at': datetime.now().isoformat()
            }
        }

    def _execute_standard_analysis(
        self,
        data: Dict[str, Any],
        session_id: str,
        progress_callback: Optional[callable] = None
    ) -> Dict[str, Any]:
        """Executa an√°lise padr√£o unificada"""

        if progress_callback:
            progress_callback(1, "üîç Iniciando an√°lise padr√£o unificada...")

        # Pesquisa unificada
        search_query = data.get('query') or f"mercado {data.get('segmento', 'neg√≥cios')} Brasil 2024"
        search_results = unified_search_manager.unified_search(search_query, context=data, session_id=session_id)

        # Extra√ß√£o de conte√∫do
        extracted_content = self._extract_unified_content(search_results, session_id)

        # An√°lise com IA
        analysis_prompt = self._build_unified_analysis_prompt(data, extracted_content)
        ai_response = ai_manager.generate_analysis(analysis_prompt, max_tokens=8192)

        if not ai_response:
            raise Exception("IA n√£o respondeu para an√°lise unificada")

        # Processa resposta
        ai_analysis = self._process_ai_response(ai_response, data)

        return {
            'tipo_analise': 'padrao_unificada',
            'projeto_dados': data,
            'pesquisa_unificada': search_results,
            'conteudo_extraido': extracted_content,
            'analise_ia': ai_analysis,
            'metadata': {
                'analysis_type': 'standard_unified',
                'session_id': session_id,
                'generated_at': datetime.now().isoformat()
            }
        }

    def _build_unified_analysis_prompt(self, data: Dict[str, Any], content: Dict[str, Any]) -> str:
        """Constr√≥i prompt unificado para an√°lise"""

        web_content = content.get('web_content', [])
        pdf_content = content.get('pdf_content', [])

        content_summary = ""

        # Resumo do conte√∫do web
        if web_content:
            content_summary += "CONTE√öDO WEB EXTRA√çDO:\n"
            for i, item in enumerate(web_content[:10], 1):
                content_summary += f"FONTE {i}: {item['title']}\n"
                content_summary += f"URL: {item['url']}\n"
                content_summary += f"Conte√∫do: {item['content'][:1500]}\n\n"

        # Resumo do conte√∫do PDF
        if pdf_content:
            content_summary += "CONTE√öDO PDF EXTRA√çDO:\n"
            for i, item in enumerate(pdf_content[:5], 1):
                content_summary += f"PDF {i}: {item['title']}\n"
                content_summary += f"P√°ginas: {item['statistics']['pages']}\n"
                content_summary += f"Conte√∫do: {item['content'][:2000]}\n\n"

        prompt = f"""
# AN√ÅLISE UNIFICADA ULTRA-DETALHADA - ARQV30 ENHANCED v2.0

Voc√™ √© o DIRETOR SUPREMO DE AN√ÅLISE UNIFICADA, especialista de elite que combina todas as metodologias.

## DADOS DO PROJETO:
- **Segmento**: {data.get('segmento', 'N√£o informado')}
- **Produto/Servi√ßo**: {data.get('produto', 'N√£o informado')}
- **P√∫blico-Alvo**: {data.get('publico', 'N√£o informado')}
- **Pre√ßo**: R$ {data.get('preco', 'N√£o informado')}
- **Objetivo de Receita**: R$ {data.get('objetivo_receita', 'N√£o informado')}

## PESQUISA UNIFICADA REALIZADA:
{content_summary[:15000]}

## GERE AN√ÅLISE UNIFICADA COMPLETA:

```json
{{
  "avatar_unificado": {{
    "nome_ficticio": "Nome espec√≠fico baseado em dados reais",
    "perfil_demografico_completo": {{
      "idade": "Faixa et√°ria com dados reais",
      "genero": "Distribui√ß√£o real",
      "renda": "Faixa de renda real",
      "escolaridade": "N√≠vel educacional real",
      "localizacao": "Regi√µes geogr√°ficas reais"
    }},
    "perfil_psicografico_profundo": {{
      "personalidade": "Tra√ßos dominantes reais",
      "valores": "Valores e cren√ßas reais",
      "comportamento_compra": "Processo real de decis√£o",
      "medos_profundos": "Medos reais documentados",
      "aspiracoes_secretas": "Aspira√ß√µes reais"
    }},
    "dores_viscerais_unificadas": [
      "Lista de 15-20 dores espec√≠ficas baseadas em dados reais"
    ],
    "desejos_secretos_unificados": [
      "Lista de 15-20 desejos profundos baseados em estudos"
    ],
    "jornada_emocional_completa": {{
      "consciencia": "Como toma consci√™ncia baseado em dados",
      "consideracao": "Processo real de avalia√ß√£o",
      "decisao": "Fatores decisivos reais",
      "pos_compra": "Experi√™ncia p√≥s-compra real"
    }}
  }},

  "posicionamento_unificado": {{
    "proposta_valor_unica": "Proposta irresist√≠vel baseada em gaps",
    "diferenciais_competitivos": [
      "Lista de diferenciais √∫nicos e defens√°veis"
    ],
    "mensagem_central": "Mensagem principal que resume tudo",
    "estrategia_oceano_azul": "Como criar mercado sem concorr√™ncia"
  }},

  "insights_unificados": [
    "Lista de 25-30 insights √∫nicos e ultra-valiosos baseados na an√°lise completa"
  ],

  "estrategia_implementacao": {{
    "fase_1_preparacao": {{
      "duracao": "Tempo necess√°rio",
      "atividades": ["Lista de atividades espec√≠ficas"],
      "investimento": "Investimento necess√°rio"
    }},
    "fase_2_execucao": {{
      "duracao": "Tempo necess√°rio", 
      "atividades": ["Lista de atividades espec√≠ficas"],
      "investimento": "Investimento necess√°rio"
    }},
    "fase_3_otimizacao": {{
      "duracao": "Tempo necess√°rio",
      "atividades": ["Lista de atividades espec√≠ficas"],
      "investimento": "Investimento necess√°rio"
    }}
  }}
}}
```

CR√çTICO: Use APENAS dados REAIS da pesquisa unificada. Combine insights de todas as fontes.
"""

        return prompt

    def _process_ai_response(self, response: str, data: Dict[str, Any]) -> Dict[str, Any]:
        """Processa resposta da IA"""

        try:
            # Extrai JSON da resposta
            clean_text = response.strip()

            if "```json" in clean_text:
                start = clean_text.find("```json") + 7
                end = clean_text.rfind("```")
                clean_text = clean_text[start:end].strip()

            # Parseia JSON
            analysis = json.loads(clean_text)

            # Adiciona metadados
            analysis['metadata_ai'] = {
                'generated_at': datetime.now().isoformat(),
                'provider_used': 'unified_ai_manager',
                'analysis_type': 'unified_complete'
            }

            return analysis

        except json.JSONDecodeError as e:
            logger.error(f"‚ùå Erro ao parsear JSON: {e}")
            return self._create_fallback_analysis(data)

    def _create_fallback_analysis(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Cria an√°lise de fallback"""

        segmento = data.get('segmento', 'neg√≥cios')

        return {
            'avatar_unificado': {
                'nome_ficticio': f'Profissional {segmento} Brasileiro',
                'dores_viscerais_unificadas': [
                    f'Trabalhar excessivamente em {segmento} sem crescimento proporcional',
                    'Sentir-se sempre atr√°s da concorr√™ncia',
                    'Ver competidores menores crescendo mais r√°pido'
                ],
                'desejos_secretos_unificados': [
                    f'Ser reconhecido como autoridade em {segmento}',
                    'Ter liberdade financeira total',
                    'Trabalhar apenas com o que ama'
                ]
            },
            'insights_unificados': [
                f'Mercado brasileiro de {segmento} em transforma√ß√£o digital',
                'Oportunidades em nichos espec√≠ficos e personalizados',
                'Necessidade de abordagem psicol√≥gica diferenciada'
            ],
            'fallback_mode': True
        }

    def _get_fallback_avatar_data(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Retorna dados de avatar de fallback se necess√°rio."""
        return {
            "nome_ficticio": f"Avatar Padr√£o para {data.get('segmento', 'Neg√≥cios')}",
            "perfil_demografico_completo": {
                "idade": "25-45 anos", "genero": "Ambos", "renda": "M√©dia-Alta",
                "escolaridade": "Superior Completo", "localizacao": "Grandes Centros Urbanos"
            },
            "perfil_psicografico_profundo": {
                "personalidade": "Pragm√°tico, busca resultados",
                "valores": "Crescimento, Seguran√ßa, Inova√ß√£o",
                "comportamento_compra": "Pesquisa online, busca por prova social",
                "medos_profundos": ["Perder dinheiro", "Ficar para tr√°s"],
                "aspiracoes_secretas": ["Lideran√ßa de mercado", "Reconhecimento profissional"]
            },
            "dores_viscerais_unificadas": [
                "Falta de clareza na estrat√©gia de marketing",
                "Resultados abaixo do esperado",
                "Dificuldade em se diferenciar da concorr√™ncia"
            ],
            "desejos_secretos_unificados": [
                "Crescimento exponencial do neg√≥cio",
                "Ser refer√™ncia no seu nicho",
                "Ter um sistema previs√≠vel de atra√ß√£o de clientes"
            ]
        }


    def _generate_unique_hash(self, analysis_data: Dict[str, Any]) -> str:
        """Gera um hash √∫nico para o conte√∫do da an√°lise, garantindo unicidade."""
        # Serializa os dados de forma consistente
        data_string = json.dumps(analysis_data, sort_keys=True, ensure_ascii=False)
        # Cria um hash SHA-256
        return hashlib.sha256(data_string.encode('utf-8')).hexdigest()


    def get_analysis_capabilities(self) -> Dict[str, Any]:
        """Retorna capacidades de an√°lise dispon√≠veis"""

        return {
            'analysis_types': self.analysis_types,
            'available_agents': {
                name: {
                    'available': True,
                    'description': f'Agente {name} dispon√≠vel'
                } for name in self.available_agents.keys()
            },
            'search_providers': unified_search_manager.get_provider_status(),
            'extraction_capabilities': {
                'web_extraction': robust_content_extractor is not None,
                'pdf_extraction': pymupdf_client.is_available(),
                'exa_neural_search': exa_client.is_available()
            },
            'ai_providers': ai_manager.get_provider_status() if ai_manager else {}
        }

# Inst√¢ncia global
unified_analysis_engine = UnifiedAnalysisEngine()